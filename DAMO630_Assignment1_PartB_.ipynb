{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2544ac",
   "metadata": {},
   "source": [
    "\n",
    "# DAMO630 — Assignment 1 (Part B Only: NYC Taxi with PySpark, FPGrowth Fixed)\n",
    "\n",
    "**Generated:** 2025-10-04 02:38  \n",
    "\n",
    "This standalone notebook contains only **Part B** with a robust FPGrowth setup:\n",
    "- B1: Download & Load TLC Yellow Taxi data (Jan 2024)\n",
    "- B2: MapReduce-equivalent in Spark (total fare per pickup zone)\n",
    "- B3: Frequent pattern mining with FPGrowth (✅ items as Array[String])\n",
    "- B4: Rider segmentation with KMeans\n",
    "- B5: Business interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install PySpark and helpers (run once per session)\n",
    "%pip install --quiet pyspark pandas numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f2357",
   "metadata": {},
   "source": [
    "## B1. Download & Load TLC Yellow Trip Data (Jan 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "parquet_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "local_path = \"yellow_tripdata_2024-01.parquet\"\n",
    "if not os.path.exists(local_path):\n",
    "    !wget -q {parquet_url} -O {local_path}\n",
    "print(\"File ready:\", local_path, os.path.getsize(local_path), \"bytes\")\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "spark = SparkSession.builder.appName(\"NYC-Taxi-PartB-Only\").getOrCreate()\n",
    "df_taxi = spark.read.parquet(local_path).cache()\n",
    "print(\"Rows:\", df_taxi.count())\n",
    "df_taxi.printSchema()\n",
    "df_taxi.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc07ed",
   "metadata": {},
   "source": [
    "## B2. MapReduce-equivalent in Spark: Total Fare per Pickup Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select & basic cleaning\n",
    "cols_needed = [\"tpep_pickup_datetime\",\"PULocationID\",\"DOLocationID\",\"trip_distance\",\"fare_amount\",\"passenger_count\"]\n",
    "df_clean = df_taxi.select(*[c for c in cols_needed if c in df_taxi.columns])\n",
    "df_clean = df_clean.where(\n",
    "    (F.col(\"fare_amount\") >= 0) &\n",
    "    (F.col(\"trip_distance\") >= 0) &\n",
    "    (F.col(\"passenger_count\") >= 0)\n",
    ")\n",
    "\n",
    "# Aggregate total fare by pickup zone\n",
    "fare_by_pu = (df_clean.groupBy(\"PULocationID\")\n",
    "                      .agg(F.sum(\"fare_amount\").alias(\"total_fare\"))\n",
    "                      .orderBy(F.desc(\"total_fare\")))\n",
    "fare_by_pu.show(10)\n",
    "\n",
    "# Save to CSV\n",
    "out_dir = \"out_fare_by_pu\"\n",
    "fare_by_pu.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(out_dir)\n",
    "print(\"Saved:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dd645",
   "metadata": {},
   "source": [
    "## B3. Frequent Travel Patterns with FPGrowth (Fixed Array[String])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# Build explicit Array[String] 'items' column\n",
    "df_basket = (df_clean\n",
    "    .withColumn(\"hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"time_bucket\",\n",
    "                F.when(F.col(\"hour\").between(6,10), F.lit(\"morning\"))\n",
    "                 .when(F.col(\"hour\").between(16,19), F.lit(\"evening\"))\n",
    "                 .otherwise(F.lit(\"other\")))\n",
    "    .withColumn(\"pickup_zone\", F.col(\"PULocationID\").cast(\"string\"))\n",
    "    .withColumn(\"dropoff_zone\", F.col(\"DOLocationID\").cast(\"string\"))\n",
    "    .select(\"pickup_zone\", \"dropoff_zone\", \"time_bucket\")\n",
    "    .na.drop()\n",
    ")\n",
    "\n",
    "df_basket = df_basket.withColumn(\"items\", F.array(\"pickup_zone\",\"dropoff_zone\",\"time_bucket\")).select(\"items\")\n",
    "\n",
    "# Optional: sample for speed (uncomment to speed up)\n",
    "# df_basket = df_basket.sample(False, 0.25, seed=42)\n",
    "\n",
    "df_basket.printSchema()\n",
    "df_basket.show(5, truncate=False)\n",
    "\n",
    "fp = FPGrowth(itemsCol=\"items\", minSupport=0.001, minConfidence=0.1)\n",
    "fp_model = fp.fit(df_basket)\n",
    "\n",
    "freq_items = fp_model.freqItemsets.orderBy(F.desc(\"freq\"))\n",
    "rules = fp_model.associationRules.orderBy(F.desc(\"lift\"))\n",
    "\n",
    "print(\"Top frequent itemsets:\")\n",
    "freq_items.show(10, truncate=False)\n",
    "\n",
    "print(\"Top association rules:\")\n",
    "rules.select(\"antecedent\",\"consequent\",\"confidence\",\"lift\",\"support\").show(10, truncate=False)\n",
    "\n",
    "# Save outputs\n",
    "freq_items.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"out_fp_itemsets\")\n",
    "rules.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"out_fp_rules\")\n",
    "print(\"Saved: out_fp_itemsets, out_fp_rules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa939e",
   "metadata": {},
   "source": [
    "## B4. Rider Segmentation with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec82183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Feature engineering\n",
    "df_seg = (df_clean\n",
    "          .withColumn(\"hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "          .select(\"trip_distance\",\"fare_amount\",\"hour\")\n",
    "          .na.drop())\n",
    "df_seg = df_seg.where((F.col(\"trip_distance\") > 0) & (F.col(\"fare_amount\") >= 0))\n",
    "\n",
    "vec = VectorAssembler(inputCols=[\"trip_distance\",\"fare_amount\",\"hour\"], outputCol=\"features_raw\")\n",
    "df_vec = vec.transform(df_seg)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "df_scaled = scaler.fit(df_vec).transform(df_vec)\n",
    "\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"features\")\n",
    "km_model = kmeans.fit(df_scaled)\n",
    "preds = km_model.transform(df_scaled).cache()\n",
    "\n",
    "print(\"Cluster centers (standardized):\")\n",
    "for i, c in enumerate(km_model.clusterCenters()):\n",
    "    print(f\"Cluster {i}: {c}\")\n",
    "\n",
    "summary = preds.groupBy(\"prediction\").agg(\n",
    "    F.count(\"*\").alias(\"n\"),\n",
    "    F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    F.avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "    F.avg(\"hour\").alias(\"avg_hour\")\n",
    ").orderBy(\"prediction\")\n",
    "summary.show()\n",
    "\n",
    "summary.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"out_kmeans_summary\")\n",
    "print(\"Saved: out_kmeans_summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d216ea",
   "metadata": {},
   "source": [
    "\n",
    "## B5. Business Interpretation (for your write-up)\n",
    "\n",
    "- **Fare by pickup zone** pinpoints revenue-dense areas (e.g., airports, midtown) — guide driver positioning and supply allocation.  \n",
    "- **Frequent patterns** (PU→DO with time buckets) support flat-fare windows, pre-positioning, and targeted promos.  \n",
    "- **KMeans segments** (short-hop commuters vs. long-distance/airport vs. late-night) inform fleet mix, pricing caps, and safety features.  \n",
    "- **Operationalization**: refresh nightly; track drift/seasonality and adjust strategies accordingly.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}